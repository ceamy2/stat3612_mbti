---
title: "Stat3616 FP"
author: "Eun Chong Chu"
date: "12/11/2021"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


```{r}
#install packages 
install.packages("ggplot2")
install.packages("reshap2")
install.packages("PerformanceAnalytics")
install.packages("remotes")
install_github("vqv/ggbiplot")
install.packages("stringr")
install.packages("factoextra")
install.packages("clValid")
install.packages("ROCR")
install.packages("pROC")
install.packages("caTools")
```

```{r}
#library 
library(ggplot2)
library(reshape2)
library("PerformanceAnalytics") 
library("remotes")
library("ggbiplot")
library(stringr) 
library(ranger)
library(naivebayes)
library(kernlab)
library(rpart)
library(factoextra)
library(clValid)
library(MASS)
library(factoextra)
library(caret)
library(ROCR)
library(pROC) 
library(caTools)
library(cluster)
```

## Exploring Cancer Dataset

```{r}
#Reading cancer.csv

cancer <- read.csv("cancer_data_original.csv", header = TRUE)
attach(cancer)


cancer.org<-cancer

cancer$diagnosis=as.numeric(cancer$diagnosis=="M")


print(cancer$diagnosis)

colnames(cancer)
original_data<- read.csv("cancer_data.csv", header = TRUE)

cancer<-as.data.frame(cancer)
cancerfactor<-cancer[,3:32]

```


######## Heatmap ########## 

```{r}
mydata<-cancer[, c(3,4,5,6,7,8,9,10,11,12)]
head(mydata)
res<-cor(mydata)
round(res,2)

#heatmap 
col<- colorRampPalette(c("navy", "white", "indianred"))(20)
heatmap(x = res, col = col, symm = TRUE)

#Create the correlation heatmap with ggplot2
#install.packages("reshape2")
melted_res<-melt(res)
head(melted_res)
ggplot(data = melted_res, aes(x=Var1, y=Var2, fill=value)) + geom_tile()
uppertriangle <- function(res){
  res[lower.tri(res)]<- NA
  return(res)}
uptri <- uppertriangle(res)
uptri
library(reshape2)
melted_res <- melt(uptri, na.rm = TRUE)
# Heatmap
library(ggplot2)
heatmap<-ggplot(data = melted_res, aes(Var2, Var1, fill = value))+ 
  geom_tile(color = "white")+
  scale_fill_gradient2(low = "navy", high = "indianred", mid = "white", 
                       midpoint = 0, limit = c(-1,1), space = "Lab", 
                       name="Pearson\nCorrelation") +
  theme_minimal()+ 
  theme(axis.text.x = element_text(angle = 45, vjust = 1, 
                                   size = 11, hjust = 1))+
  coord_fixed()

# install.packages("ggmap")
library(ggmap)
# melted_res <- ggmap(melted_res)
heatmap + 
geom_text(aes(Var2, Var1, label = round(value,digits=2)), color = "black", size = 2) +
theme(
  axis.title.x = element_blank(),
  axis.title.y = element_blank(),
  panel.grid.major = element_blank(),
  panel.border = element_blank(),
  panel.background = element_blank(),
  axis.ticks = element_blank(),
  legend.justification = c(1, 0),
  legend.position = c(0.6, 0.7),
  legend.direction = "horizontal")+
  guides(fill = guide_colorbar(barwidth = 7, barheight = 1,
                title.position = "top", title.hjust = 0.5))


```

#Reformatting Data
```{r}

mydata<-cancer[, 3:12]
head(mydata)
res<-cor(mydata)
round(res,2)
```

#VIF 
```{r}
#install.packages("magrittr") 
library(magrittr) 
#install.packages("caret")
library(caret)
#install.packages("car")
library(car)

# Split the data into training and test set
cancerVIF<-cancer[,2:13]
set.seed(123)
training.samples <- createDataPartition(y=cancerVIF$diagnosis,p=0.8,list = FALSE)
train.data  <- cancerVIF[training.samples, ]
test.data <- cancerVIF[-training.samples, ]

# Build the model
model1 <- lm(diagnosis ~., data = train.data)
# Make predictions
predictions <- model1 %>% predict(test.data)
# Model performance
data.frame(
  RMSE = RMSE(predictions, test.data$diagnosis),
  R2 = R2(predictions, test.data$diagnosis)
)
car::vif(model1)
```





### peformming PCA 

```{r}

##use entire variables 
cancerfactor<-cancer[,3:32]


##calculating mean and variance of possible cancer factors 
round(colMeans(cancerfactor),4)
round(apply(cancerfactor,2,sd),4) 
#By this we recognize the neccessity of scaling the data when working on pca 

##creating pca 
cancer.pca <- prcomp(cancerfactor, scale = T, center=TRUE)

```


#plotting pca plot 
```{r}
# this needs cancer$diagnosis :M/B
# this one shows the separation of the data
fviz_pca_ind(cancer.pca, geom.ind = "point", pointshape=21, pointsize=2, fill.ind=cancer.org$diagnosis, addEllipses = TRUE, label= "var", col.ind = "black", col.var="black", paleete="jco", legend.title ="Diagnosis" ) + ggtitle("two dimensional PCA plot based on 30 features")+ theme(plot.title = element_text(hjust=0.5))


#this one shows the vector 
ggbiplot(cancer.pca)
summary(cancer.pca) 

```

reference : 
https://towardsdatascience.com/principal-component-analysis-pca-101-using-r-361f4c53a9ff 



# identify how much variance are explained through pca components 

```{r}

# Calculate variability of each component

cancerfactor.var<-cancer.pca$sdev^2 
cancer.pve <- cancerfactor.var / sum(cancerfactor.var)


## can choose either one but 2nd one proves the knowledge of pca that 1st pca component has the highest eignevalue  
# Plot variance explained for each principal component
#1
plot(cancer.pve, xlab = "Principal Component", 
     ylab = "Proportion of Variance Explained", 
     ylim = c(0, 1), type = "b")

#2
fviz_eig(cancer.pca)
#pca component 1 explains the most variance.



# Plot cumulative proportion of variance explained
plot(cumsum(cancer.pve), xlab = "Principal Component", 
     ylab = "Cumulative Proportion of Variance Explained", 
     ylim = c(0, 1), type = "b")

#to explain 90% of the varaiance need 7pca component
 

```


## model selection: LDA. KNN. RF. Decision tree. NB... 

#use same tuning controls and compare the best model with highest AUC value 



### splitting data 

```{r}

set.seed(12L)


cancer.org.mod=cancer.org[,2:32]
trainingIndex <-createDataPartition(cancer.org$diagnosis, p= 0.75, list=FALSE)
trainingData<-cancer.org.mod[trainingIndex,]
testData<-cancer.org.mod[-trainingIndex,]


```


### LDA Analysis

```{r}

#cancer.data
cancerfactor
lda_df <- lda(diagnosis ~., data = trainingData, scale = TRUE)


pca_ldf<- lda(diagnosis~.,data=trainingData)


prop.lda<-lda_df$sdev^2/sum(lda_df$svd^2)

library(scales)
plda<-predict(lda_df,newdata=testData)


## evaluation 

cancer.predict.post<-as.data.frame(plda$posterior)

pred<-prediction(cancer.predict.post[,2],testData$diagnosis)

roc.perf = performance(pred,measure ="tpr",x.measure = "fpr")

auc.train<-performance(pred,measure="auc") 
auc.train<-auc.train@y.values


### area under roc curve
plot(roc.perf)+abline(a=0,b=1)+text(x=0.25,y=0.65, paste("AUC=",round(auc.train[[1]],3)),sep="")
print(auc.train[[1]],3)


```

#LDA analysis using PCA 
```{r}
#### data preparation 
cancer.pcst<-cancer.pca$x[,1:6]
cancer.pcst<- cbind(cancer.pcst,cancer$diagnosis)
colnames(cancer.pcst)[7]<-"diagnosis"


##spliting data
set.seed(12L)
trainingIndex <-createDataPartition(diagnosis, p= 0.75, list=FALSE)
trainingDatalda<-cancer.pcst[trainingIndex,]
testDatalda<-cancer.pcst[-trainingIndex,]
trainingDatalda<-as.data.frame(trainingDatalda)
testDatalda<-as.data.frame(testDatalda)


cancer.lda <- lda(diagnosis~.,data=trainingDatalda)
cancer.lda.predict<-predict(cancer.lda,newdata=testDatalda) 

############ evaulation
cancer.predict.post<-as.data.frame(cancer.lda.predict$posterior)

pred<-prediction(cancer.predict.post[,2],testDatalda$diagnosis)

roc.perf = performance(pred,measure ="tpr",x.measure = "fpr")

auc.train<-performance(pred,measure="auc") 
auc.train<-auc.train@y.values


##ROC curve 
plot(roc.perf)+abline(a=0,b=1)+text(x=0.25,y=0.65, paste("AUC=",round(auc.train[[1]],3)),sep="")
 
```
reference : 
https://towardsdatascience.com/linear-discriminant-analysis-lda-101-using-r-6a97217a55a6 

If there is too much things to write can only use lda analysis using PCA 

first one is just normal lda and second one is lda using pca components 
lda with pca components have slightely higher AUC than normal lda  





### splitting data 

```{r}

set.seed(12L)


cancer.org.mod=cancer.org[,2:32]
trainingIndex <-createDataPartition(cancer.org$diagnosis, p= 0.75, list=FALSE)
trainingData<-cancer.org.mod[trainingIndex,]
testData<-cancer.org.mod[-trainingIndex,]


```


### machine learning tech (idk if it is supervised or unsupervised)

#0. setting the tuning parameter 

```{r}


set.seed(12L)

myFolds <- createFolds(trainingData$diagnosis, k = 5)
str(myFolds)

mycontrol<-trainControl(
                        method="cv", 
                        number = 10,
                        classProbs=TRUE, 
                        savePredictions = TRUE,                           summaryFunction=twoClassSummary, 
                        index=myFolds,
                        verboseIter = FALSE)

```

#1 knn method 
```{r}


set.seed(12L)
fit.knn<-train(diagnosis~.,
               data=trainingData,
               method="knn",
               metric="ROC",
               tuneLength=10,
               preProc=c("center","scale"),
               trControl = mycontrol) 

prediction_pknn<- predict(fit.knn, testData, type = "prob")
colAUC(prediction_pknn,testData$diagnosis, plotROC = TRUE) 

## another method  to evaluate using confusion matrix
prediction_pknn<- predict(fit.knn, testData)
confusionMatrix(prediction_pknn, factor(testData$diagnosis))


```
the r console is the AUC value. 


## 2. random forest 
```{r}

set.seed(12L)
fit.randomforest<-train(diagnosis~.,
                        data=trainingData,method = "ranger", 
                        metric="ROC", 
                        tuneLength=10,
                        tuneGrid = expand.grid(
                        mtry = c(3,4,5,6),
                        splitrule = c("gini", "extratrees"),
                        min.node.size = 1),
                        trControl = mycontrol) 


prediction_prf<- predict(fit.randomforest, testData, type = "prob")
colAUC(prediction_prf, testData$diagnosis, plotROC = TRUE)

## another method  to evaluate using confusion matrix
prediction_prf<- predict(fit.randomforest, testData)
confusionMatrix(prediction_prf, factor(testData$diagnosis))

```


## 3.Naives Bayes  


```{r}

set.seed(12L)
fit.nb<- train(diagnosis~.,
               data=trainingData,
               method="naive_bayes",
               metric="ROC",
               tuneLength=10,
               trControl=mycontrol)

prediction_pnb<- predict(fit.nb, testData, type = "prob")
colAUC(prediction_pnb, testData$diagnosis, plotROC = TRUE)

## another method  to evaluate using confusion matrix
prediction_pnb<- predict(fit.nb, testData)
confusionMatrix(prediction_pnb, factor(testData$diagnosis))


```


## 4. svm 

```{r}



set.seed(12L)
fit.svm<-train(diagnosis~.,
               data=trainingData,
               method="svmRadial",
               metric="ROC", 
               tuneLength=10, 
               trControl=mycontrol)


prediction_psvm<- predict(fit.svm, testData, type = "prob")
colAUC(prediction_psvm, testData$diagnosis, plotROC = TRUE)

## another method  to evaluate using confusion matrix
prediction_psvm<- predict(fit.svm, testData)
confusionMatrix(prediction_psvm, factor(testData$diagnosis))


```

```{r}


model_list <- list( 
                   knn = fit.knn,
                   rf = fit.randomforest,
                   nb = fit.nb,
                   svm= fit.svm
                   )

resamp <- resamples(model_list)

summary(resamp)
lattice::bwplot(resamp, metric = "ROC")
lattice::bwplot(resamp, metric = "Sens")
lattice::bwplot(resamp, metric = "Spec")

```
Result: ROC SVM is best model for this data set. which is quite different from the kaggle notebook lol 


```{r}

#############Preparation of the data for clustering 
set.seed(12L)
cancer.pca$rotation[1:10,1:2]
#Since the value of each varaible far apart, need to be scaled 
cancerfactor.scaled <- scale(cancerfactor) 


# Calculate the (Euclidean) distances: data.dist
cancerfactor.dist <- dist(cancerfactor.scaled)
```




### Additional modelling : clustering
unsupervised algorithm 
```{r}

set.seed(50)
# Create a hierarchical clustering model: wisc.hclust
cancerfactor.hclust <- hclust(cancerfactor.dist, method = "complete")


# Results of hierarchical clustering
# cutting the height at 20 will give 4 clusters
plot(cancerfactor.hclust)

## Selecting number of clusters
# Cut tree so that it has 4 clusters: wisc.hclust.clusters
cancerfactor.hclust.clusters <- cutree(cancerfactor.hclust, k = 4)

cancerfactor.hclust.clusters
# Compare cluster membership to actual diagnoses
table(cancerfactor.hclust.clusters, diagnosis)

# count out of place observations based on cluster 
# basically just summing the row mins here
sum(apply(table(cancerfactor.hclust.clusters, diagnosis), 1, min))  
## the minority of the each clusters added 


###############################################


cancerfactor.pr.hclust <- hclust(dist(cancer.pca$x[, 1:2]), method = "complete")

# Cut model into 4 clusters: wisc.pr.hclust.clusters
cancerfactor.pr.hclust.clusters <- cutree(cancerfactor.pr.hclust, k = 4)


###
clusplot(scale(cancerfactor), cancerfactor.pr.hclust.clusters, main='2D representation of the Cluster solution',
         color=TRUE, shade=TRUE,plotchar=TRUE,
         labels=1, lines=2) 
### not appropriate becuz 4 clusters. If you want to directly compare can use k =2 and run this code 

dev.off

###########################################################


##kmeans of clustering 
cancerfactor.km <- kmeans(scale(cancerfactor), centers = 2, nstart = 20)


clusplot(scale(cancerfactor), cancerfactor.km$cluster, main='2D representation of the Cluster solution',
         color=TRUE, shade=TRUE,plotchar=TRUE,
         labels=1, lines=2)
dev.off
##idk how to change the color of the points under different shades 


# Compare to actual diagnoses
t <- table(cancerfactor.pr.hclust.clusters, diagnosis)
t
 
sum(apply(t, 1, min))
t <- table(cancerfactor.km$cluster, diagnosis)
t

sum(apply(t, 1, min))


############ 

#other method to evaluate : dunns 

dunn_km<-dunn(clusters=cancerfactor.km$cluster,Data=cancerfactor)

dunn_km

dunn_hclust<-dunn(clusters=cancerfactor.hclust.clusters,Data=cancerfactor)

dunn_hclust

dunn_prhclust<-dunn(clusters=cancerfactor.pr.hclust.clusters,Data=cancerfactor)

dunn_prhclust

```

can get the dunn index to compare among the clustering 

The table is the confusion matrix for clustering. Since clustering's accuracy cannot be obtained through ROC, can use confusion matrix to compare with other models 









